#!/bin/bash
set -euo pipefail

# =============================================================================
# Unified Deployment Script for Streaming Pipeline
# =============================================================================
# Supports all 4 combinations:
# - Storage: garage | s3
# - Format: paimon | iceberg
#
# Usage: ./deploy [--storage=garage|s3] [--format=paimon|iceberg] [--build]
# =============================================================================

# --- Host Configuration ---
REMOTE_SERVER="calvo@192.168.1.202"
REMOTE_DIR="/mefitis/streaming"

# --- Default Configuration ---
STORAGE_TYPE="garage"
DATALAKE_FORMAT="paimon"
BUILD_FLAG=""

# --- Parse command line arguments ---
for arg in "$@"; do
    case $arg in
        --storage=*)
            STORAGE_TYPE="${arg#*=}"
            if [[ "$STORAGE_TYPE" != "garage" && "$STORAGE_TYPE" != "s3" ]]; then
                echo "‚ùå ERROR: Invalid storage type '$STORAGE_TYPE'. Use 'garage' or 's3'."
                exit 1
            fi
            ;;
        --format=*)
            DATALAKE_FORMAT="${arg#*=}"
            if [[ "$DATALAKE_FORMAT" != "paimon" && "$DATALAKE_FORMAT" != "iceberg" ]]; then
                echo "‚ùå ERROR: Invalid format '$DATALAKE_FORMAT'. Use 'paimon' or 'iceberg'."
                exit 1
            fi
            ;;
        --build)
            BUILD_FLAG="--build"
            ;;
        *)
            echo "‚ùå ERROR: Unknown argument: $arg"
            echo "Usage: $0 [--storage=garage|s3] [--format=paimon|iceberg] [--build]"
            exit 1
            ;;
    esac
done

echo "üöÄ Starting Unified Deployment Script..."
echo "üóÑÔ∏è  Storage Backend: $STORAGE_TYPE"
echo "üìä Data Lake Format: $DATALAKE_FORMAT"
if [ -n "$BUILD_FLAG" ]; then
    echo "üî® Force Build: Enabled"
fi

# --- Environment Setup ---
echo "üîß Setting up environment configuration..."

# Load default values from template
if [ ! -f ".env.template" ]; then
    echo "‚ùå ERROR: .env.template not found. This file is required for configuration."
    exit 1
fi

# Start with clean .env from template
cp .env.template .env

# Set the primary configuration
echo "STORAGE_TYPE=$STORAGE_TYPE" >> .env
echo "DATALAKE_FORMAT=$DATALAKE_FORMAT" >> .env

if [ "$STORAGE_TYPE" = "garage" ]; then
    echo "üè† Configuring for Garage storage backend..."

    # Garage-specific settings
    echo "S3_ENDPOINT=http://192.168.1.202:3900" >> .env
    echo "S3_BUCKET=warehouse" >> .env
    echo "S3_PATH_STYLE_ACCESS=true" >> .env

    # Access keys will be generated by garage setup
    echo "S3_ACCESS_KEY=" >> .env
    echo "S3_SECRET_KEY=" >> .env

elif [ "$STORAGE_TYPE" = "s3" ]; then
    echo "‚òÅÔ∏è  Configuring for S3 storage backend..."

    # Check if user has provided S3 configuration
    if [ ! -f ".env" ] || ! grep -q "S3_ACCESS_KEY=" .env || ! grep -q "S3_SECRET_KEY=" .env; then
        echo "‚ùå ERROR: For S3 storage, please configure your .env file with:"
        echo "   S3_ENDPOINT=https://s3.amazonaws.com"
        echo "   S3_ACCESS_KEY=your-access-key"
        echo "   S3_SECRET_KEY=your-secret-key"
        echo "   S3_BUCKET=your-bucket-name"
        echo "   S3_PATH_STYLE_ACCESS=false  # false for real AWS S3"
        exit 1
    fi
fi

# Compute derived paths
S3_BASE="s3://$(grep S3_BUCKET .env | cut -d'=' -f2)"
echo "S3_BASE=$S3_BASE" >> .env
echo "PAIMON_WAREHOUSE=${S3_BASE}/paimon" >> .env
echo "ICEBERG_WAREHOUSE=${S3_BASE}/iceberg" >> .env
echo "CHECKPOINTS_DIR=${S3_BASE}/checkpoints" >> .env

# Format-specific configuration
if [ "$DATALAKE_FORMAT" = "paimon" ]; then
    echo "üìã Configuring for Paimon (JDBC metastore)..."
    # Paimon configurations will be used, Iceberg configs will be empty
    echo "PAIMON_METASTORE=jdbc" >> .env
    echo "PAIMON_URI=jdbc:postgresql://\${HOST_IP}:\${POSTGRES_CATALOG_PORT}/\${POSTGRES_CATALOG_DB}" >> .env
    echo "PAIMON_USER=\${POSTGRES_CATALOG_USER}" >> .env
    echo "PAIMON_PASSWORD=\${POSTGRES_CATALOG_PASSWORD}" >> .env
    echo "PAIMON_CATALOG_KEY=paimon_catalog" >> .env

    # Empty Iceberg configs
    echo "ICEBERG_TYPE=" >> .env
    echo "ICEBERG_WAREHOUSE_PATH=" >> .env
    echo "ICEBERG_ENDPOINT=" >> .env
    echo "ICEBERG_ACCESS_KEY=" >> .env
    echo "ICEBERG_SECRET_KEY=" >> .env
    echo "ICEBERG_PATH_STYLE=" >> .env

elif [ "$DATALAKE_FORMAT" = "iceberg" ]; then
    echo "‚ùÑÔ∏è  Configuring for Iceberg (Hadoop catalog)..."
    # Iceberg configurations will be used, Paimon configs will be empty
    echo "ICEBERG_TYPE=hadoop" >> .env
    echo "ICEBERG_WAREHOUSE_PATH=\${ICEBERG_WAREHOUSE}" >> .env
    echo "ICEBERG_ENDPOINT=\${S3_ENDPOINT}" >> .env
    echo "ICEBERG_ACCESS_KEY=\${S3_ACCESS_KEY}" >> .env
    echo "ICEBERG_SECRET_KEY=\${S3_SECRET_KEY}" >> .env
    echo "ICEBERG_PATH_STYLE=\${S3_PATH_STYLE_ACCESS}" >> .env

    # Empty Paimon configs
    echo "PAIMON_METASTORE=" >> .env
    echo "PAIMON_URI=" >> .env
    echo "PAIMON_USER=" >> .env
    echo "PAIMON_PASSWORD=" >> .env
    echo "PAIMON_CATALOG_KEY=" >> .env
fi

echo "üì§ 1. Syncing Local Changes to Remote Server..."
bash sync.sh

echo "üåê 2. Connecting to $REMOTE_SERVER for Remote Environment Reset..."

if [ "$STORAGE_TYPE" = "garage" ]; then
    # Garage setup - need to provision garage and generate credentials
    ssh -t "$REMOTE_SERVER" "sudo nix-shell -p jq --run '
        cd $REMOTE_DIR
        set -euo pipefail

        echo \"-----------------------------------\"
        echo \"------- SUBSTEP: CLEANUP ----------\"
        echo \"-----------------------------------\"
        echo \"üõë Stopping all services...\"

        docker compose -f flink-cdc/docker-compose.yaml down -v || true
        docker compose -f fluss/docker-compose.yaml down -v || true
        docker compose -f garage/docker-compose.yaml down -v || true
        docker compose -f postgres-catalog/docker-compose.yaml down -v || true
        docker compose -f postgres-source/docker-compose.yaml down -v || true

        echo \"üßπ Pruning Docker volumes...\"
        docker volume prune -f

        echo \"-----------------------------------------\"
        echo \"------- SUBSTEP: DATA DELETION ----------\"
        echo \"-----------------------------------------\"
        echo \"üóëÔ∏è Deleting ALL data directories...\"
        find . -name \"data\" -type d -exec rm -rf {} +

        echo \"üßπ Removing old .env file before provisioning...\"
        rm -f garage/.env

        echo \"----------------------------------------\"
        echo \"------- SUBSTEP: INFRA REBOOT ----------\"
        echo \"----------------------------------------\"
        echo \"üêò Starting Foundation Services...\"

        # Always start source postgres and garage
        docker compose -f postgres-source/docker-compose.yaml up -d
        docker compose -f garage/docker-compose.yaml up -d

        # Only start catalog postgres for Paimon (Iceberg uses Hadoop catalog)
        if [ \"$DATALAKE_FORMAT\" = \"paimon\" ]; then
            echo \"üìã Starting PostgreSQL catalog for Paimon...\"
            docker compose -f postgres-catalog/docker-compose.yaml up -d
        else
            echo \"‚ùÑÔ∏è  Skipping PostgreSQL catalog for Iceberg (uses Hadoop catalog)...\"
        fi

        echo \"‚è≥ Waiting 5s for Garage S3 API...\"
        sleep 5

        echo \"-----------------------------------------\"
        echo \"------- SUBSTEP: GARAGE CONFIG ----------\"
        echo \"-----------------------------------------\"
        echo \"üîë Provisioning Garage S3 buckets and keys...\"

        cd garage && bash ./garage.sh
    '"

    echo "üì• 3. Fetching Credentials and Updating Local Environment..."
    # Fetch the freshly generated .env from the server back to your local machine
    scp "$REMOTE_SERVER:$REMOTE_DIR/garage/.env" ./garage/.env

    if [ -f "garage/.env" ]; then
        # Load garage credentials and update our .env
        source garage/.env

        # Update .env with garage credentials
        sed -i '' "s/S3_ACCESS_KEY=.*/S3_ACCESS_KEY=${GARAGE_ACCESS_KEY}/" .env
        sed -i '' "s/S3_SECRET_KEY=.*/S3_SECRET_KEY=${GARAGE_SECRET_KEY}/" .env

        echo "‚úÖ Garage credentials updated in .env"
    else
        echo "‚ùå ERROR: garage/.env was not fetched. Aborting." && exit 1
    fi

else
    # S3 setup - just clean up existing services (no garage)
    ssh -t "$REMOTE_SERVER" "sudo nix-shell -p jq --run '
        cd $REMOTE_DIR
        set -euo pipefail

        echo \"-----------------------------------\"
        echo \"------- SUBSTEP: CLEANUP ----------\"
        echo \"-----------------------------------\"
        echo \"üõë Stopping all services...\"

        docker compose -f flink-cdc/docker-compose.yaml down -v || true
        docker compose -f fluss/docker-compose.yaml down -v || true
        docker compose -f postgres-catalog/docker-compose.yaml down -v || true
        docker compose -f postgres-source/docker-compose.yaml down -v || true

        echo \"üßπ Pruning Docker volumes...\"
        docker volume prune -f

        echo \"-----------------------------------------\"
        echo \"------- SUBSTEP: DATA DELETION ----------\"
        echo \"-----------------------------------------\"
        echo \"üóëÔ∏è Deleting ALL data directories...\"
        find . -name \"data\" -type d -exec rm -rf {} +

        echo \"----------------------------------------\"
        echo \"------- SUBSTEP: INFRA REBOOT ----------\"
        echo \"----------------------------------------\"
        echo \"üêò Starting Foundation Services (S3 mode)...\"

        # Always start source postgres
        docker compose -f postgres-source/docker-compose.yaml up -d

        # Only start catalog postgres for Paimon (Iceberg uses Hadoop catalog)
        if [ \"$DATALAKE_FORMAT\" = \"paimon\" ]; then
            echo \"üìã Starting PostgreSQL catalog for Paimon...\"
            docker compose -f postgres-catalog/docker-compose.yaml up -d
        else
            echo \"‚ùÑÔ∏è  Skipping PostgreSQL catalog for Iceberg (uses Hadoop catalog)...\"
        fi

        echo \"‚è≥ Waiting 3s for PostgreSQL services...\"
        sleep 3
    '"
fi

echo "üì§ 4. Executing Local Sync and Starting Remote Jobs..."
bash ./sync.sh

# Load environment variables for remote execution
source .env

ssh -t "$REMOTE_SERVER" "sudo bash -c '
    cd $REMOTE_DIR

    # Load environment variables
    source .env

    echo \"-------------------------------------------\"
    echo \"------- SUBSTEP: STREAMING LAYER ----------\"
    echo \"-------------------------------------------\"

    echo \"üåä Starting Fluss & üêøÔ∏è Flink CDC...\"
    docker compose -f fluss/docker-compose.yaml up -d $BUILD_FLAG
    docker compose -f flink-cdc/docker-compose.yaml up -d $BUILD_FLAG

    echo \"‚è≥ Waiting for Flink JobManager to go live...\"
    until curl -s http://localhost:8081/overview > /dev/null; do sleep 2; done

    echo \"--------------------------------------------\"
    echo \"------- SUBSTEP: REPLICATION TASK ----------\"
    echo \"--------------------------------------------\"
    echo -e \"\nüöÄ Running SQL Client: Postgres -> Fluss...\"

    echo \"üìä Starting CDC for users table...\"
    docker exec -it flink-sql-client \\
      /opt/flink/bin/sql-client.sh -f /opt/flink/sql/users-cdc.sql

    echo \"üé¨ Starting CDC for movies table...\"
    docker exec -it flink-sql-client \\
      /opt/flink/bin/sql-client.sh -f /opt/flink/sql/movies-cdc.sql

    echo \"üé´ Starting CDC for tickets table...\"
    docker exec -it flink-sql-client \\
      /opt/flink/bin/sql-client.sh -f /opt/flink/sql/tickets-cdc.sql

    echo \"üìä Starting revenue analytics job...\"
    docker exec -it flink-sql-client \\
      /opt/flink/bin/sql-client.sh -f /opt/flink/sql/revenue-analytics.sql

    echo \"--------------------------------------------\"
    echo \"------- SUBSTEP: Tiering service ----------\"
    echo \"--------------------------------------------\"
    echo \"üì¶ Starting Fluss Tiering Service...\"

    if [ \"\$DATALAKE_FORMAT\" = \"paimon\" ]; then
        docker exec flink-jobmanager /opt/flink/bin/flink run \\
          -Dpipeline.name=\"Fluss Tiering Service\" \\
          -Dparallelism.default=4 \\
          -Dexecution.checkpointing.interval=30s \\
          -Dstate.checkpoints.dir=\"\${CHECKPOINTS_DIR}/tiering\" \\
          -Ds3.multiobjectdelete.enable=false \\
          -Dtaskmanager.memory.network.fraction=0.2 \\
          -Dtaskmanager.memory.managed.fraction=0.6 \\
          /opt/flink/lib/fluss-flink-tiering-0.8.0-incubating.jar \\
          --fluss.bootstrap.servers \${HOST_IP}:9123 \\
          --datalake.format paimon \\
          --datalake.paimon.metastore jdbc \\
          --datalake.paimon.uri \"jdbc:postgresql://\${HOST_IP}:\${POSTGRES_CATALOG_PORT}/\${POSTGRES_CATALOG_DB}\" \\
          --datalake.paimon.jdbc.user \${POSTGRES_CATALOG_USER} \\
          --datalake.paimon.jdbc.password \${POSTGRES_CATALOG_PASSWORD} \\
          --datalake.paimon.catalog-key paimon_catalog \\
          --datalake.paimon.warehouse \"\${PAIMON_WAREHOUSE}\" \\
          --datalake.paimon.s3.endpoint \"\$S3_ENDPOINT\" \\
          --datalake.paimon.s3.access-key \"\$S3_ACCESS_KEY\" \\
          --datalake.paimon.s3.secret-key \"\$S3_SECRET_KEY\" \\
          --datalake.paimon.s3.path.style.access \$S3_PATH_STYLE_ACCESS
    else
        docker exec flink-jobmanager /opt/flink/bin/flink run \\
          -Dpipeline.name=\"Fluss Tiering Service\" \\
          -Dparallelism.default=4 \\
          -Dexecution.checkpointing.interval=30s \\
          -Dstate.checkpoints.dir=\"\${CHECKPOINTS_DIR}/tiering\" \\
          -Ds3.multiobjectdelete.enable=false \\
          -Dtaskmanager.memory.network.fraction=0.2 \\
          -Dtaskmanager.memory.managed.fraction=0.6 \\
          /opt/flink/lib/fluss-flink-tiering-0.8.0-incubating.jar \\
          --fluss.bootstrap.servers \${HOST_IP}:9123 \\
          --datalake.format iceberg \\
          --datalake.iceberg.type hadoop \\
          --datalake.iceberg.warehouse \"\${ICEBERG_WAREHOUSE}\" \\
          --datalake.iceberg.hadoop.fs.s3a.endpoint \"\$S3_ENDPOINT\" \\
          --datalake.iceberg.hadoop.fs.s3a.access.key \"\$S3_ACCESS_KEY\" \\
          --datalake.iceberg.hadoop.fs.s3a.secret.key \"\$S3_SECRET_KEY\" \\
          --datalake.iceberg.hadoop.fs.s3a.path.style.access \$S3_PATH_STYLE_ACCESS
    fi
'"

echo -e "\n‚ú® ALL STEPS COMPLETE!"
echo "üóÑÔ∏è  Storage Backend: $STORAGE_TYPE"
echo "üìä Data Lake Format: $DATALAKE_FORMAT"
if [ "$STORAGE_TYPE" = "garage" ]; then
    echo "üè† Data stored in Garage at: $(grep S3_BASE .env | cut -d'=' -f2)"
else
    echo "‚òÅÔ∏è  Data stored in S3 at: $(grep S3_BASE .env | cut -d'=' -f2)"
fi